{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import trimesh\n",
    "\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "from dust3r.inference import inference, load_model\n",
    "from dust3r.image_pairs import make_pairs\n",
    "from dust3r.utils.image import load_images, rgb\n",
    "from dust3r.utils.device import to_numpy\n",
    "from dust3r.viz import add_scene_cam, CAM_COLORS, OPENGL, pts3d_to_trimesh, cat_meshes\n",
    "from dust3r.cloud_opt import global_aligner, GlobalAlignerMode\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "pl.ion()\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # for gpu >= Ampere and pytorch >= 1.12\n",
    "batch_size = 1\n",
    "import pandas as pd\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ade20K = pd.read_csv(\n",
    "    \"/scratch2/yuxili/interiorDesign/color_coding_semantic_segmentation_classes - Sheet1.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    0: \"wall\",\n",
    "    1: \"building\",\n",
    "    2: \"sky\",\n",
    "    3: \"floor\",\n",
    "    4: \"tree\",\n",
    "    5: \"ceiling\",\n",
    "    6: \"road\",\n",
    "    7: \"bed \",\n",
    "    8: \"windowpane\",\n",
    "    9: \"grass\",\n",
    "    10: \"cabinet\",\n",
    "    11: \"sidewalk\",\n",
    "    12: \"person\",\n",
    "    13: \"earth\",\n",
    "    14: \"door\",\n",
    "    15: \"table\",\n",
    "    16: \"mountain\",\n",
    "    17: \"plant\",\n",
    "    18: \"curtain\",\n",
    "    19: \"chair\",\n",
    "    20: \"car\",\n",
    "    21: \"water\",\n",
    "    22: \"painting\",\n",
    "    23: \"sofa\",\n",
    "    24: \"shelf\",\n",
    "    25: \"house\",\n",
    "    26: \"sea\",\n",
    "    27: \"mirror\",\n",
    "    28: \"rug\",\n",
    "    29: \"field\",\n",
    "    30: \"armchair\",\n",
    "    31: \"seat\",\n",
    "    32: \"fence\",\n",
    "    33: \"desk\",\n",
    "    34: \"rock\",\n",
    "    35: \"wardrobe\",\n",
    "    36: \"lamp\",\n",
    "    37: \"bathtub\",\n",
    "    38: \"railing\",\n",
    "    39: \"cushion\",\n",
    "    40: \"base\",\n",
    "    41: \"box\",\n",
    "    42: \"column\",\n",
    "    43: \"signboard\",\n",
    "    44: \"chest of drawers\",\n",
    "    45: \"counter\",\n",
    "    46: \"sand\",\n",
    "    47: \"sink\",\n",
    "    48: \"skyscraper\",\n",
    "    49: \"fireplace\",\n",
    "    50: \"refrigerator\",\n",
    "    51: \"grandstand\",\n",
    "    52: \"path\",\n",
    "    53: \"stairs\",\n",
    "    54: \"runway\",\n",
    "    55: \"case\",\n",
    "    56: \"pool table\",\n",
    "    57: \"pillow\",\n",
    "    58: \"screen door\",\n",
    "    59: \"stairway\",\n",
    "    60: \"river\",\n",
    "    61: \"bridge\",\n",
    "    62: \"bookcase\",\n",
    "    63: \"blind\",\n",
    "    64: \"coffee table\",\n",
    "    65: \"toilet\",\n",
    "    66: \"flower\",\n",
    "    67: \"book\",\n",
    "    68: \"hill\",\n",
    "    69: \"bench\",\n",
    "    70: \"countertop\",\n",
    "    71: \"stove\",\n",
    "    72: \"palm\",\n",
    "    73: \"kitchen island\",\n",
    "    74: \"computer\",\n",
    "    75: \"swivel chair\",\n",
    "    76: \"boat\",\n",
    "    77: \"bar\",\n",
    "    78: \"arcade machine\",\n",
    "    79: \"hovel\",\n",
    "    80: \"bus\",\n",
    "    81: \"towel\",\n",
    "    82: \"light\",\n",
    "    83: \"truck\",\n",
    "    84: \"tower\",\n",
    "    85: \"chandelier\",\n",
    "    86: \"awning\",\n",
    "    87: \"streetlight\",\n",
    "    88: \"booth\",\n",
    "    89: \"television receiver\",\n",
    "    90: \"airplane\",\n",
    "    91: \"dirt track\",\n",
    "    92: \"apparel\",\n",
    "    93: \"pole\",\n",
    "    94: \"land\",\n",
    "    95: \"bannister\",\n",
    "    96: \"escalator\",\n",
    "    97: \"ottoman\",\n",
    "    98: \"bottle\",\n",
    "    99: \"buffet\",\n",
    "    100: \"poster\",\n",
    "    101: \"stage\",\n",
    "    102: \"van\",\n",
    "    103: \"ship\",\n",
    "    104: \"fountain\",\n",
    "    105: \"conveyer belt\",\n",
    "    106: \"canopy\",\n",
    "    107: \"washer\",\n",
    "    108: \"plaything\",\n",
    "    109: \"swimming pool\",\n",
    "    110: \"stool\",\n",
    "    111: \"barrel\",\n",
    "    112: \"basket\",\n",
    "    113: \"waterfall\",\n",
    "    114: \"tent\",\n",
    "    115: \"bag\",\n",
    "    116: \"minibike\",\n",
    "    117: \"cradle\",\n",
    "    118: \"oven\",\n",
    "    119: \"ball\",\n",
    "    120: \"food\",\n",
    "    121: \"step\",\n",
    "    122: \"tank\",\n",
    "    123: \"trade name\",\n",
    "    124: \"microwave\",\n",
    "    125: \"pot\",\n",
    "    126: \"animal\",\n",
    "    127: \"bicycle\",\n",
    "    128: \"lake\",\n",
    "    129: \"dishwasher\",\n",
    "    130: \"screen\",\n",
    "    131: \"blanket\",\n",
    "    132: \"sculpture\",\n",
    "    133: \"hood\",\n",
    "    134: \"sconce\",\n",
    "    135: \"vase\",\n",
    "    136: \"traffic light\",\n",
    "    137: \"tray\",\n",
    "    138: \"ashcan\",\n",
    "    139: \"fan\",\n",
    "    140: \"pier\",\n",
    "    141: \"crt screen\",\n",
    "    142: \"plate\",\n",
    "    143: \"monitor\",\n",
    "    144: \"bulletin board\",\n",
    "    145: \"shower\",\n",
    "    146: \"radiator\",\n",
    "    147: \"glass\",\n",
    "    148: \"clock\",\n",
    "    149: \"flag\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_scene_output_to_glb(outdir, out_name, imgs, pts3d, mask, focals, cams2world, cam_size=0.05,\n",
    "                                 cam_color=None, as_pointcloud=False, transparent_cams=False):\n",
    "    assert len(pts3d) == len(mask) <= len(imgs) <= len(cams2world) == len(focals)\n",
    "    pts3d = to_numpy(pts3d)\n",
    "    imgs = to_numpy(imgs)\n",
    "    focals = to_numpy(focals)\n",
    "    cams2world = to_numpy(cams2world)\n",
    "    \n",
    "    scene = trimesh.Scene()\n",
    "\n",
    "    pts = np.concatenate([p[m] for p, m in zip(pts3d, mask)])\n",
    "    col = np.concatenate([p[m] for p, m in zip(imgs, mask)])\n",
    "    pct = trimesh.PointCloud(pts.reshape(-1, 3), colors=col.reshape(-1, 3))\n",
    "    scene.add_geometry(pct)\n",
    "    pct.export(outdir + out_name)\n",
    "\n",
    "def get_3D_model_from_scene(outdir, out_name, scene, min_conf_thr=3, as_pointcloud=False, mask_sky=False,\n",
    "                            clean_depth=False, transparent_cams=False, cam_size=0.05):\n",
    "    \"\"\"\n",
    "    extract 3D_model (glb file) from a reconstructed scene\n",
    "    \"\"\"\n",
    "    if scene is None:\n",
    "        return None\n",
    "    # post processes\n",
    "    if clean_depth:\n",
    "        scene = scene.clean_pointcloud()\n",
    "    if mask_sky:\n",
    "        scene = scene.mask_sky()\n",
    "\n",
    "    # get optimized values from scene\n",
    "    rgbimg = scene.imgs\n",
    "    focals = scene.get_focals().cpu()\n",
    "    cams2world = scene.get_im_poses().cpu()\n",
    "    # 3D pointcloud from depthmap, poses and intrinsics\n",
    "    pts3d = to_numpy(scene.get_pts3d())\n",
    "    scene.min_conf_thr = float(scene.conf_trf(torch.tensor(min_conf_thr)))\n",
    "    msk = to_numpy(scene.get_masks())\n",
    "    return _convert_scene_output_to_glb(outdir, out_name, rgbimg, pts3d, msk, focals, cams2world, as_pointcloud=as_pointcloud,\n",
    "                                        transparent_cams=transparent_cams, cam_size=cam_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/scratch2/yuxili/interiorDesign/dust3r/checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\"\n",
    "device = \"cuda\"\n",
    "batch_size = 1\n",
    "schedule = \"cosine\"\n",
    "lr = 0.01\n",
    "niter = 300\n",
    "image_path = \"/scratch2/yuxili/interiorDesign/output/livingroom/2024-04-10-19-56-08-experiment/room.png\"\n",
    "\n",
    "model = load_model(model_path, device)\n",
    "# load_images can take a list of images or a directory\n",
    "images = load_images(\n",
    "    [\n",
    "        image_path,image_path\n",
    "        # \"/scratch2/yuxili/interiorDesign/output/livingroom/2024-04-10-19-56-08-experiment/inpaint/0.png\",\n",
    "        # \"/scratch2/yuxili/interiorDesign/output/livingroom/2024-04-10-19-56-08-experiment/inpaint/1.png\",\n",
    "        # \"/scratch2/yuxili/interiorDesign/output/livingroom/2024-04-10-19-56-08-experiment/inpaint/3.png\",\n",
    "        # \"/scratch2/yuxili/interiorDesign/output/livingroom/2024-04-10-19-56-08-experiment/inpaint/4.png\",\n",
    "        # \"/scratch2/yuxili/interiorDesign/output/livingroom/2024-04-10-19-56-08-experiment/inpaint/6.png\",\n",
    "        # \"/scratch2/yuxili/interiorDesign/output/livingroom/2024-04-10-19-56-08-experiment/inpaint/7.png\",\n",
    "        # \"/scratch2/yuxili/interiorDesign/output/livingroom/2024-04-10-19-56-08-experiment/inpaint/8.png\",\n",
    "    ],\n",
    "    size=512,\n",
    ")\n",
    "pairs = make_pairs(images, scene_graph=\"complete\", prefilter=None, symmetrize=True)\n",
    "output = inference(pairs, model, device, batch_size=batch_size)\n",
    "\n",
    "scene = global_aligner(\n",
    "    output, device=device, mode=GlobalAlignerMode.PointCloudOptimizer\n",
    ")\n",
    "loss = scene.compute_global_alignment(init=\"mst\", niter=niter, schedule=schedule, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsics1 = np.zeros((4, 4))\n",
    "intrinsics1[:3, :3] = scene.get_intrinsics()[0].cpu().detach().numpy()\n",
    "intrinsics1[3, 3] = 1\n",
    "\n",
    "intrinsics2 = np.zeros((4, 4))\n",
    "intrinsics2[:3, :3] = scene.get_intrinsics()[1].cpu().detach().numpy()\n",
    "intrinsics2[3, 3] = 1\n",
    "\n",
    "intrinsics = (intrinsics1 + intrinsics2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve useful values from scene:\n",
    "imgs = scene.imgs\n",
    "focals = scene.get_focals()\n",
    "poses = scene.get_im_poses()\n",
    "pts3d = scene.get_pts3d()\n",
    "confidence_masks = scene.get_masks()\n",
    "cams2world = scene.get_im_poses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_3D_model_from_scene(\n",
    "    \"../../point_cloud/\",\n",
    "    \"test_render_2_inpaint.ply\",\n",
    "    scene,\n",
    "    min_conf_thr=3,\n",
    "    as_pointcloud=True,\n",
    "    mask_sky=False,\n",
    "    clean_depth=True,\n",
    "    transparent_cams=False,\n",
    "    cam_size=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud = o3d.io.read_point_cloud(\"/scratch2/yuxili/interiorDesign/output/livingroom/2024-04-10-19-56-08-experiment/room.ply\")\n",
    "cl, ind = point_cloud.remove_radius_outlier(nb_points=50, radius=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.io.write_point_cloud(\"output.ply\", cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloud:\n",
    "    def __init__(self, point_cloud_path):\n",
    "        pcd = o3d.io.read_point_cloud(point_cloud_path)\n",
    "        self.points = np.asarray(pcd.points)\n",
    "        self.num_points = self.points.shape[0]\n",
    "        self.colors = np.asarray(pcd.colors)\n",
    "\n",
    "    def get_homogeneous_coordinates(self):\n",
    "        return np.append(self.points, np.ones((self.num_points, 1)), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud = PointCloud(\"../../point_cloud/test.ply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poses = cams2world[0].cpu().detach().numpy()\n",
    "poses = np.eye(4)\n",
    "X = point_cloud.get_homogeneous_coordinates()\n",
    "n_points = point_cloud.num_points\n",
    "depth = scene.get_depthmaps()[0].cpu().detach().numpy()\n",
    "intrinsic = intrinsics\n",
    "\n",
    "projected_points = np.zeros((n_points, 2), dtype=int)\n",
    "visible_points_view = np.zeros((n_points), dtype=bool)\n",
    "print(f\"[INFO] Computing the visible points in each view.\")\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# STEP 1: get the projected points\n",
    "# Get the coordinates of the projected points in the i-th view (i.e. the view with index idx)\n",
    "projected_points_not_norm = (intrinsic @ poses @ X.T).T\n",
    "# Get the mask of the points which have a non-null third coordinate to avoid division by zero\n",
    "mask = projected_points_not_norm[:, 2] != 0\n",
    "\n",
    "# don't do the division for point with the third coord equal to zero\n",
    "# Get non homogeneous coordinates of valid points (2D in the image)\n",
    "\n",
    "projected_points[mask] = np.column_stack(\n",
    "    [\n",
    "        [\n",
    "            projected_points_not_norm[:, 0][mask]\n",
    "            / projected_points_not_norm[:, 2][mask],\n",
    "            projected_points_not_norm[:, 1][mask]\n",
    "            / projected_points_not_norm[:, 2][mask],\n",
    "        ]\n",
    "    ]\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\n",
    "    \"/scratch2/yuxili/interiorDesign/output/bedroom/2024-04-02-13-25-53-experiment/bedroom.png\"\n",
    ")\n",
    "# resize image\n",
    "cache_dir = \"/scratch2/yuxili/interiorDesign/huggingface/\"\n",
    "\n",
    "processor = OneFormerProcessor.from_pretrained(\n",
    "    \"shi-labs/oneformer_ade20k_swin_large\", cache_dir=cache_dir\n",
    ")\n",
    "model = OneFormerForUniversalSegmentation.from_pretrained(\n",
    "    \"shi-labs/oneformer_ade20k_swin_large\", cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "# Semantic Segmentation\n",
    "semantic_inputs = processor(images=image, task_inputs=[\"semantic\"], return_tensors=\"pt\")\n",
    "semantic_outputs = model(**semantic_inputs)\n",
    "# pass through image_processor for postprocessing\n",
    "predicted_semantic_map = processor.post_process_semantic_segmentation(\n",
    "    semantic_outputs, target_sizes=[image.size[::-1]]\n",
    ")[0]\n",
    "\n",
    "predicted_semantic_map = predicted_semantic_map.float()  # Convert tensor to float\n",
    "predicted_semantic_map = predicted_semantic_map.unsqueeze(0).unsqueeze(0)\n",
    "downsampled_map = F.interpolate(\n",
    "    predicted_semantic_map, size=(512, 512), mode=\"bilinear\", align_corners=False\n",
    ")\n",
    "predicted_semantic_map = downsampled_map.squeeze(0).squeeze(0).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_color = point_cloud.colors\n",
    "\n",
    "for idx, point in enumerate(projected_points):\n",
    "    pc_color[idx] = np.array(\n",
    "        [\n",
    "            int(element)\n",
    "            for element in ade20K.iloc[\n",
    "                int(predicted_semantic_map.T[point[0], point[1]])\n",
    "            ][\"Color_Code (R,G,B)\"]\n",
    "            .strip(\"()\")\n",
    "            .split(\",\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "point_cloud.colors = pc_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "# Assuming 'points' and 'colors' are your arrays containing the point coordinates and colors\n",
    "points = np.asarray(point_cloud.points)\n",
    "colors = np.asarray(point_cloud.colors) / point_cloud.colors.max()\n",
    "\n",
    "# Create a PointCloud object\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "# Assign the points and colors to the PointCloud object\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "# Save the point cloud to a file\n",
    "o3d.io.write_point_cloud(\"output.ply\", pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clolor_map = np.zeros((517, 517, 3), dtype=np.uint8)\n",
    "for idx, point in enumerate(projected_points):\n",
    "    clolor_map[point[0], point[1], :] = point_cloud.colors[idx] * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(clolor_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(projected_points[:, 0].min())\n",
    "# print(projected_points[:, 0].max())\n",
    "# print(projected_points[:, 1].min())\n",
    "# print(projected_points[:, 1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(predicted_semantic_map.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _convert_scene_output_to_glb(\n",
    "#     outdir,\n",
    "#     out_name,\n",
    "#     imgs,\n",
    "#     pts3d,\n",
    "#     mask,\n",
    "#     focals,\n",
    "#     cams2world,\n",
    "#     cam_size=0.05,\n",
    "#     cam_color=None,\n",
    "#     as_pointcloud=False,\n",
    "#     transparent_cams=False,\n",
    "# ):\n",
    "#     assert len(pts3d) == len(mask) <= len(imgs) <= len(cams2world) == len(focals)\n",
    "#     pts3d = to_numpy(pts3d)\n",
    "#     imgs = to_numpy(imgs)\n",
    "#     focals = to_numpy(focals)\n",
    "#     cams2world = to_numpy(cams2world)\n",
    "\n",
    "#     scene = trimesh.Scene()\n",
    "\n",
    "#     pts = np.concatenate([p[m] for p, m in zip(pts3d, mask)])\n",
    "#     col = np.concatenate([p[m] for p, m in zip(imgs, mask)])\n",
    "#     pct = trimesh.PointCloud(pts.reshape(-1, 3), colors=col.reshape(-1, 3))\n",
    "\n",
    "#     pct.export(outdir + out_name)\n",
    "#     scene.add_geometry(pct)\n",
    "\n",
    "#     # add each camera\n",
    "#     for i, pose_c2w in enumerate(cams2world):\n",
    "#         if isinstance(cam_color, list):\n",
    "#             camera_edge_color = cam_color[i]\n",
    "#         else:\n",
    "#             camera_edge_color = cam_color or CAM_COLORS[i % len(CAM_COLORS)]\n",
    "#         add_scene_cam(\n",
    "#             scene,\n",
    "#             pose_c2w,\n",
    "#             camera_edge_color,\n",
    "#             None if transparent_cams else imgs[i],\n",
    "#             focals[i],\n",
    "#             imsize=imgs[i].shape[1::-1],\n",
    "#             screen_width=cam_size,\n",
    "#         )\n",
    "\n",
    "\n",
    "# def get_3D_model_from_scene(\n",
    "#     outdir,\n",
    "#     out_name,\n",
    "#     scene,\n",
    "#     min_conf_thr=3,\n",
    "#     as_pointcloud=False,\n",
    "#     mask_sky=False,\n",
    "#     clean_depth=False,\n",
    "#     transparent_cams=False,\n",
    "#     cam_size=0.05,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     extract 3D_model (glb file) from a reconstructed scene\n",
    "#     \"\"\"\n",
    "#     if scene is None:\n",
    "#         return None\n",
    "#     # post processes\n",
    "#     if clean_depth:\n",
    "#         scene = scene.clean_pointcloud()\n",
    "#     if mask_sky:\n",
    "#         scene = scene.mask_sky()\n",
    "\n",
    "#     # get optimized values from scene\n",
    "#     rgbimg = scene.imgs\n",
    "#     focals = scene.get_focals().cpu()\n",
    "#     cams2world = scene.get_im_poses().cpu()\n",
    "#     # 3D pointcloud from depthmap, poses and intrinsics\n",
    "#     pts3d = to_numpy(scene.get_pts3d())\n",
    "#     scene.min_conf_thr = float(scene.conf_trf(torch.tensor(min_conf_thr)))\n",
    "#     msk = to_numpy(scene.get_masks())\n",
    "#     return _convert_scene_output_to_glb(\n",
    "#         outdir,\n",
    "#         out_name,\n",
    "#         rgbimg,\n",
    "#         pts3d,\n",
    "#         msk,\n",
    "#         focals,\n",
    "#         cams2world,\n",
    "#         as_pointcloud=as_pointcloud,\n",
    "#         transparent_cams=transparent_cams,\n",
    "#         cam_size=cam_size,\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
